%!Tex Root = ../main.tex
% ./Packete.tex
% ./Design.tex
% ./Vorbereitung.tex
% ./Aufgabe1.tex
% ./Aufgabe2.tex
% ./Aufgabe3.tex
% ./Aufgabe4.tex
% ./Appendix.tex

\begin{mindmap}
  \begin{mindmapcontent}
    \node (ml) at (current page.center) {Machine Learning
      \resizebox{\textwidth}{!}{
        \begin{minipage}[t]{20cm}
          \begin{itemize}
            \item ML \alert{replaces the agent} with a math function $f$ 
            \begin{itemize}
              \item an agent whose context are \alert{features} $x$ and its decision is the \alert{target} $y$
              \item \alert{trains} $f$ from \alert{observations} $\{(x_1 , y_1), \ldots , (x_N , y_N )\}$  such that $f(x_i) \approx y_i$, $\forall i \in \{1, \ldots , N\}$
            \end{itemize}
          \end{itemize}
        \end{minipage}
      }
    }
    child {
      node {Supervised Learning
        \resizebox{\textwidth}{!}{
          \begin{minipage}[t]{12cm}
            \begin{itemize}
              \item Target is given by an expert (ground-truth)
              \item Classification, Regression, Ranking
              \item \alert{Archetype:}
              \begin{itemize}
                \item \alert{data dimensions:} $N$ instances having $M$ features
                \item \alert{features:} $x \in \mathbb{R}^{N\times M}$ and \alert{target:} $y \in \mathbb{R}^N$
                \item \alert{prediction model:} having parameters $ \theta \in R^k$ is $f:\mathbb{R}^M\times \mathbb{R}^K \rightarrow \mathbb{R}$
                \begin{itemize}
                  \item $\hat y_n := f(x_n, \theta)$, function that predicts the target
                \end{itemize}
                \item \alert{loss function:} $\mathcal{L}(y_n, \hat y_n): \mathbb{R}\times\mathbb{R}\rightarrow \mathbb{R}$
                \begin{itemize}
                  \item quality of a prediction model $f(x, \theta)$
                  \item difference between the \alert{estimated target} $\hat y$ and \alert{ground-truth target} $y$
                  \item the term loss is used for minimization tasks, e.g. regression, sometimes a maximization of $\mathcal{L}(y, \hat y)$ is needed
                \end{itemize}
                \item \alert{regularisation:} $\Omega(\theta): \mathbb{R}^k \rightarrow \mathbb{R}$ fights overfitting to the noise of the measured data
                \item \alert{objective function:} $\displaystyle \underset{\theta}{argmin} \sum^N_{n=1} \mathcal{L}(y_n, \hat y_n) + \Omega(\theta)$
              \end{itemize}
            \end{itemize}
          \end{minipage}
        }
      }
      child {
        node {Prediction Models}
        child {
          node {Linear Model
            \resizebox{\textwidth}{!}{
              \begin{minipage}[t]{10cm}
                \begin{itemize}
                  \item $\displaystyle \hat{y}_n=\theta_0+\theta_1 x_{n, 1}+\theta_2 x_{n, 2}+\cdots+\theta_M x_{n, M}=\theta_0+\sum_{m=1}^M \theta_m x_{n, m}$
                \end{itemize}
              \end{minipage}
            }
          }
          child {
            node (lr) {Linear Regression
              \resizebox{\textwidth}{!}{
                \begin{minipage}[t]{8cm}
                  \begin{itemize}
                    \item $f\left(\mathbf{x}_i ; \mathbf{w}\right)=w_0+w_1 x_{i, 1}+\ldots+w_D x_{i, D}=w_0+w^T x_i$
                    \item non-linear function can be approximated by a linear function if one maps the feature x into a new dimensionality with a \alert{basis function}: $\phi: \mathbb{R}^D\rightarrow \mathbb{R}^M$
                    \item \alert{enhanced linear regression model:} linear combinations of fixed \alert{nonlinear functions} of the input variables: $\displaystyle f(x_i, \omega) = \omega_0 + \sum^{M}_{j=1} \omega_j\phi(x_i)_j$
                    \begin{itemize}
                      \item $f(x, w)$ is still a \alert{linear function} of the weights / parameters $w_i$
                      \item $f(x, w)$ is a \alert{nonlinear} function of the input dimensions / variables $x_i$
                    \end{itemize}
                  \end{itemize}
                \end{minipage}
              }
            }
            child {
              node {Analytical solution
                \resizebox{\textwidth}{!}{
                  \begin{minipage}[t]{8cm}
                    \begin{itemize}
                      \item setting the \alert{partial derivatives} w.r.t. $w$ to zero: $\displaystyle\dfrac{\partial}{\partial \omega} \sum^N_{n=1} \mathcal{L}(y_n, f_n(x, \omega)) = 0$
                      \item solving for $\omega$
                    \end{itemize}
                  \end{minipage}
                }
              }
            }
          }
          child {
            node (lc) {Linear Classification
              \resizebox{\textwidth}{!}{
                \begin{minipage}[t]{8cm}
                  \begin{itemize}
                    \item using \alert{logistic regression} which bounds the output of linear regression to $0 \le h_w(x) \le 1$
                    \begin{itemize}
                      \item e.g. $h_w(x) = g(w^Tx)$ with \alert{sigmoid} function or \alert{logistic function} $\displaystyle g(z) = \frac{1}{1 + e^{-z}}$ introduces \alert{non-linearity}
                      \item $h_w(x) = P(y=1|x; \omega)$, estimated probability, that $y = 1$
                    \end{itemize}
                  \end{itemize}
                \end{minipage}
              }
            }
            child {
              node {Gradient Descent
                \resizebox{\textwidth}{!}{
                  \begin{minipage}[t]{8cm}
                    \begin{itemize}
                      \item  if there is \alert{no analytic solution}
                      \item start with a guess for $\omega$
                      \item move $\omega$ towards the minimum of $J(\omega)$
                      \item minimize $J(\omega)$ by updating $\omega$ in the negative direction of $\dfrac{\partial J(\omega)}{\partial\omega}$: $\displaystyle\omega^{next} \leftarrow \omega^{prev} - \eta \frac{\partial J(\omega)}{\partial\omega^{prev}} $
                    \end{itemize}
                  \end{minipage}
                }
              }
              child {
                node {Gradients
                  \resizebox{\textwidth}{!}{
                    \begin{minipage}[t]{8cm}
                      \begin{itemize}
                        \item for \alert{linear regression}
                        \item for \alert{logistic regression}
                      \end{itemize}
                    \end{minipage}
                  }
                }
              }
              child {
                node {Stochastic Gradient Descent
                  \resizebox{\textwidth}{!}{
                    \begin{minipage}[t]{8cm}
                      \begin{itemize}
                        \item for \alert{large dimensions}
                        \item nice if the error function is \alert{convex}
                      \end{itemize}
                    \end{minipage}
                  }
                }
              }
            }
          }
        }
        child {
          node {Polynomial Regression
            \resizebox{\textwidth}{!}{
              \begin{minipage}[t]{10cm}
                \begin{itemize}
                  \item $\displaystyle\hat{y}_n=\theta_0+\sum_{m=1}^M \theta_m x_{n, m}+\sum_{m=1}^M \sum_{m^{\prime}=1}^M \theta_{m, m^{\prime}} x_{n, m} x_{n, m^{\prime}}+\ldots$
                \end{itemize}
              \end{minipage}
            }
          }
          child {
            node {Decision Trees}
            child {
              node {as a Prediction Model}
            }
            child {
              node {as a Step-wise Function}
            }
          }
          child {
            node {Neural Networks
              \resizebox{\textwidth}{!}{
                \begin{minipage}[t]{8cm}
                  \begin{itemize}
                    \item Composite functions, i.e., functions of functions
                    \item A neuron indexed i is a non-linear function $f_i(x, \theta_i)$
                    \item If neuron $i$ is connected to neuron $j$ the model is $f_j (f_i (x, \theta_i ), \theta_j)$
                  \end{itemize}
                \end{minipage}
              }
            }
          }
        }
      }
      child {
        node {Generalization Performance
          \resizebox{\textwidth}{!}{
            \begin{minipage}[t]{8cm}
              \begin{itemize}
                \item \alert{Overfitting} (High model variance): Model perfectly fits the training data (incl. noise). Captering noise.
                \item \alert{Underfitting} (High model bias): Model fails to fit the training data. Unable to capture complexity
                \item \alert{Generalization}: Model is accurate on test data
              \end{itemize}
            \end{minipage}
          }
        }
      }
      child {
        node {Loss Functions}
        child {
          node {Regression
            \resizebox{\textwidth}{!}{
              \begin{minipage}[t]{8cm}
                \begin{itemize}
                  \item target is real-valued $y_n\in \mathbb{R}$
                  \item \alert{Least-squares:} $\mathcal{L}(y_n, \hat y_n) := (y_n - \hat y_n)^2$
                  \item \alert{L1:} $\mathcal{L}(y_n, \hat y_n) := |y_n - \hat y_n|$
                \end{itemize}
              \end{minipage}
            }
          }
        }
        child {
          node (bc) {Binary Classification
            \resizebox{\textwidth}{!}{
              \begin{minipage}[t]{8cm}
                \begin{itemize}
                  \item \alert{Logistic loss}, $y_n\in\{0, 1\}$:
                  \begin{itemize}
                    \item $J\left(h_{\mathbf{w}}\left(y_i, \mathbf{x}_i\right)\right)=\left\{\begin{array}{rll}
-\log \left(h_{\mathbf{w}}\left(\mathbf{x}_i\right)\right) & \text { for } & y_i=1 \\
-\log \left(1-h_{\mathbf{w}}\left(\mathbf{x}_i\right)\right) & \text { for } & y_i=0 \end{array}\right)$
                    \item \alert{avoid case destinction:}\\ $\mathcal{L}(y_n, \hat y_n) := -y_n log(\hat y_n) - (1 - y_n)log(1-\hat y_n)$
                    \begin{itemize}
                      \item is \alert{convex}, but there's no analytic solution
                    \end{itemize}
                  \end{itemize}
                  \item \alert{Hinge loss}, $y_n\in\{-1,1\}$:
                  \begin{itemize}
                    \item $\mathcal{L}(y_n, \hat y_n) := max(0, 1 - y_n\hat y_n)$
                  \end{itemize}
                \end{itemize}
              \end{minipage}
            }
          }
        }
        child {
          node {Multi-class Classification
            \resizebox{\textwidth}{!}{
              \begin{minipage}[t]{8cm}
                \begin{itemize}
                  \item Re-express targets $y_n \in \{1, \ldots , C\}$ as one-vs-all, i.e. $y_{n, c} := \begin{cases}
                      1 & y_n = C \\
                      0 & y_n \neq C
                  \end{cases}$
                  \item learn model parameters per class $\theta \in \mathbb{R}^{C\times K}$
                  \item Estimations expressed as probabilities among classes [...]
                  \item Logloss [...]
                \end{itemize}
              \end{minipage}
            }
          }
        }
      }
    }
    child {
      node {Unsupervised Learning
        \resizebox{\textwidth}{!}{
          \begin{minipage}[t]{8cm}
            \begin{itemize}
              \item Target contains no explicit labels of the context features
              \item Clustering, Dimensionality reduction, Anomaly/Outlier Detection
            \end{itemize}
          \end{minipage}
        }
      }
    }
    child {
      node {Probability theory}
      child {
        node {Bayes Rule
          \resizebox{\textwidth}{!}{
            \begin{minipage}[t]{8cm}
              \begin{itemize}
                \item $\begin{aligned}[t]
P(y \mid x) & =P(y) \frac{P(x \mid y)}{P(x)} \\
& =P(y) \frac{P(x \mid y)}{\sum_{y^{\prime} \in \mathcal{Y}} P\left(x \mid y^{\prime}\right) P\left(y^{\prime}\right)}
\end{aligned}$
              \end{itemize}
            \end{minipage}
          }
        }
        child {
          node {Naive Bayes
            \resizebox{\textwidth}{!}{
              \begin{minipage}[t]{10cm}
                \begin{itemize}
                  \item assume $x_1, x_2 \ldots, x_M$ are all independent given $y$ :\\ $\begin{aligned}[t]
P\left(y \mid x_1, x_2 \ldots, x_M\right) & =P(y) \frac{P\left(x_1, x_2 \ldots, x_M \mid y\right)}{P\left(x_1, x_2 \ldots, x_M\right)} \\
& =P(y) \frac{P\left(x_1 \mid y\right) P\left(x_2 \mid y\right) \ldots P\left(x_M \mid y\right)}{P\left(x_1, x_2 \ldots, x_M\right)}
\end{aligned}$
                  \item if the goal is only to predict $y$ we can drop the denominator:\\ $P\left(y \mid x_1, x_2 \ldots, x_M\right) \propto P(y) P\left(x_1 \mid y\right) P\left(x_2 \mid y\right) \ldots P\left(x_M \mid y\right)$
                \end{itemize}
              \end{minipage}
            }
          }
        }
      }
      child {
        node {Maximum Likelihood Estimation
          \resizebox{\textwidth}{!}{
            \begin{minipage}[t]{12cm}
              \begin{itemize}
                \item \alert{likelihood} of observing the \alert{target} $y \in \mathbb{R}^N$ is $\displaystyle \mathcal{L}(\theta) = \prod^N_{n=1} \hat p(y_n | x_n, \theta)$
                \begin{itemize}
                  \item $\hat p(y |x, \theta)$ is the \alert{probability density function} for the target $y$ given features $x$ and parameters $\theta$
                  \item assume the error in predicting the ground truth yn is normally distributed: $\epsilon | x \sim \mathcal{N}(0, \sigma^2)$, so $\hat y \sim \mathcal{N}(\theta_0 + \sum^M_{m=1} \theta_m x_m, \sigma^2)$, because of the linear model: $\displaystyle \hat y = \theta_0 + \sum^M_{m=1} \theta_m x_m$
                \end{itemize}
                \item \alert{Aim:} Estimate the $\theta$-s which maximize the likelihood
                \begin{itemize}
                  \item $\underset{\theta}{\operatorname{argmax}}\;g(\theta)=\underset{\theta}{\operatorname{argmax}}\;\log (g(\theta))$
                  \item $\displaystyle log\;\mathcal{L}(\theta) = \log \prod_{n=1}^N \hat{p}\left(y_n \mid \theta\right)=\sum_{n=1}^N \log \left(\hat{p}\left(y_n \mid \theta\right)\right)$
                \end{itemize}
              \end{itemize}
            \end{minipage}
          }
        }
      }
    }
    child {
      node {Machine Learning Design Cycle}
      child {
        node {1. preprocessing}
      }
      child {
        node {2. feature extraction / encoding}
      }
      child {
        node {3. feature selection}
      }
      child {
        node {4. machine learning}
      }
      child {
        node {5. evaluation \& model selection}
      }
      child {
        node {6. postprocessing}
      }
    };
  \end{mindmapcontent}
  \begin{edges}
    \edge{lc}{bc}
  \end{edges}
  \annotation{ml.south}{This mindmap is provided without guarantee of correctness and completeness!};
  \annotation{lr.south}{
    \resizebox{\textwidth}{!}{
      \begin{minipage}[t]{8cm}
        \alert{Regression:} Estimating the relationships between a dependent variable (\alert{label}) and one or more independent variables (\alert{features})
      \end{minipage}
  }};
\end{mindmap}
