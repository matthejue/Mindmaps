\documentclass{standalone}

\input{./content/packages}
\input{./content/desgin}
\input{./content/declarations}

\begin{document}
\begin{mindmap}
	\begin{mindmapcontent}
		\node (middle) at (current page.center) {Machine Learning}
		child {
				node {Supervised Learning}
				child {
						node (target) {Instances, Features, Target}
						child {
								node {Estimated target $\hat y$}
							}
						child {
								node {Ground-truth target $y$}
							}
					}
				child {
						node {Prediction Models}
						child {
								node {Linear Model
										% \resizebox{\textwidth}{!}{
										% 	\begin{minipage}[t]{12cm}
										% 		\begin{itemize}
										% 			\item cannot capture complex interactions between different features
										% 		\end{itemize}
										% 	\end{minipage}
										% }
									}
								child {
										node {Derive weight parameters}
										child {
												node {Analytical solution}
												child {
														node {3 assumptions}
													}
											}
										child {
												node {Iterative Optimization}
												child {
														node {Gradient Descent}
														child {
																node {Stochastic Gradient Descent}
															}
													}
                        child {
                          node {Gradients}
                          child {
                            node {Linear Regression}
                          }
                          child {
                            node {Logistic Regression}
                          }
                        }
											}
									}
								child {
										node {Regression}
										child {
												node {Basis function (for nonlinear mapping)}
												child {
														node {Characteristics of enhanced linear regression model}
													}
												child {
														node {Quality of model}
														child {
																node {L1 loss}
															}
														child {
																node {L2 loss}
															}
														child {
																node {Huber loss}
															}
													}
											}
									}
								child {
										node {Classification}
										child {
												node {Logistic regressio}
												child {
														node {Sigmoid / logistic function}
													}
											}
										child {
												node {Loss function for logistic regression}
												child {
														node {Formulation avoiding case distinction}
													}
												child {
														node {Good property: Is convex}
													}
												child {
														node {Bad property: No analytic solution}
														child {
																node {But gradient descent works well}
															}
													}
											}
									}
							}
						child {
								node {Polynomial Regression
										% \resizebox{\textwidth}{!}{
										% 	\begin{minipage}[t]{12cm}
										% 		\begin{itemize}
										% 			\item all these terms called monimials
										% 			\item can capture relation between features
										% 			\item high cofficient downplay inportance of combnation of features
										% 		\end{itemize}
										% 	\end{minipage}
										% }
									}
							}
						child {
								node {Decision Trees}
							}
						child {
								node {Neural Networks}
							}
						child {
								node {Non-parametric Models
										% \resizebox{\textwidth}{!}{
										% 	\begin{minipage}[t]{12cm}
										% 		\begin{itemize}
										% 			\item non-parametric models in contrast to the models that um we covered in the previous lecture content um we are not going to have prediction models that have parameters Theta instead the prediction models that we are going to see next are models that do not train Theta instead that they look directly at the data that we have collected in the past in order to make prediction so they are going to exploit the data and the logic is going to be based directly by Computing some information from the data instead of accumulating the knowledge into the parameters
										% 			\item such type of models that do not have their own parameters but instead they look up the data they are called otherwise as lazy models in the sense that they are lazy they don't train their own parameters but they look up the data directly
										% 			\item should understand that both nearest neighbor and naive bayes just did computations on the data
										% 			\item so that is different to for instance neural networks that have weights and that try to memorize things such type of models that we saw such as nearest neighbors and naive bayes they don't have memory they just look look into the values they do computations on the fly that's it that's how those nonparametric models work
										% 		\end{itemize}
										% 	\end{minipage}
										% }
									}
								child {
										node {Nearest Neighbors
												% \resizebox{\textwidth}{!}{
												% 	\begin{minipage}[t]{16cm}
												% 		\begin{itemize}
												% 			\item the logic of the nearest neighbor classifier is that it looks up all the training examples in your data set and finds the ones that are the most similar to the instance for which you want for which you want to make a prediction
												% 			\item so U you find the K nearest neighbors of X and you aggregate the target variables of those instances and when we say we want to find the nearest neighbors it means we want to find the neighbors that have the smallest distance and the distance is measured by a function D that takes as an input two feature vectors and computes their actual inverse of the similarities so the smaller is the D the more the more similar data points are
												% 			\item so basically we take the most likely option here which is one
												% 			\item we say that that the prediction is one because we computed the similarity of this new value nine to all the previous values in our recording data data set and that we found out that the most similar ones have a target of one so our prediction is one
												% 			\item compute the distance between this point and all the points
												% 			\item in this case it can be for example be the euclidean distance or square of the euclidean distance or even the absolute value can be used
												% 			\item that shows you the sensitivity actually of the nearest neighbor algorithm to the setting of how many neighbors you use
												% 			\item k for example is a hyper parameter based on the way you select K you are going to have different results in terms of your prediction and then based on that you are going to have different accuracies of your prediction
												% 			\item has motivations into way how humans reason, if don't know much about domain try to remember similar situations in the past
												% 			\item nearest neighbor is not always accurate and we saw that it is very sensitive to the choice of K and it is not immune to outliers
												% 			\item D is actually a set of tuples and the tuples have two vectors the feature and the target
												% 			\item so X is in the dimensionality of M many features y if it is a multi categorical can be a vector of one hot encoded Target values or it can be a single binary value or a single continuous value if it is a continuous variable
												% 			\item but in essence our data is composed of features and targets for n many instances that we have collected in the past
												% 			\item there is no other combination of three elements which are going to have a smaller sum than the ones that have the smallest distance so the sum of the three smallest values is always the subset of the data where the total sum is the smallest possible
												% 			\item so that's just a math definition of saying give me the instances X that achieve X actually the smallest possible distance to an item for which we want to make a prediction
												% 			\item hausing good example, most similiar hauses based on features of the house, e.g. compute euclidean distance between this feature vector
												%                   \item y nearest of x is just projection y into the y elements of the tuples of the nearest neighbors s of x
												% 			\item short summary: finds similar examples and predicts based on them
												% 		\end{itemize}
												% 		\begin{center}
												% 			\color{SwitchColor}\rule{14cm}{0.25mm}
												% 		\end{center}
												% 		\begin{itemize}
												% 			\item nearest neighbor iterated over training examples and count the most nearest neighbors
												% 		\end{itemize}
												% 	\end{minipage}
												% }
											}
										child {
												node {Aggregation for continuous target
														% \resizebox{\textwidth}{!}{
														% 	\begin{minipage}[t]{12cm}
														% 		\begin{itemize}
														%                     \item for continous target aggregation is for every element $\hat y$ in the set of the target values for the nearest neighbor of x that is basically the average of those values
														% 			\item if those three targets are in fact continuous values then the aggregation will take the average of them
														% 		\end{itemize}
														% 	\end{minipage}
														% }
													}
											}
										child {
												node {Aggregation for nominal targets (class c)
														% \resizebox{\textwidth}{!}{
														% 	\begin{minipage}[t]{12cm}
														% 		\begin{itemize}
														% 			\item nominal / categorical variable
														% 			\item want to make prediction for every class
														% 			\item for evey class prediction is basically: we say how many of the nearest neighbors y' belong to this category (cardinality of set of items that match the class) and we device it by the number of nearest neighbors
														% 			\item der mittlere | ist ein fuer die gilt ':'
														% 		\end{itemize}
														% 	\end{minipage}
														% }
													}
											}
									}
								child {
										node {Naive Bayes
												% \resizebox{\textwidth}{!}{
												% 	\begin{minipage}[t]{14cm}
												% 		\begin{itemize}
												% 			\item problem, this exact example never appears in our data, neither for positive nor negative class of covid
												% 			\item if this term is 0, then this probability becomes 0
												% 			\item so problem is: if have lots of variables then the chance of having coverage of all possible combinations becomes even smaller that have coverage of all possible combinations
												% 			\item example, 10 binary variables, then $2^{10}$ combinations of values, chance of having $2^{10}$ many patients and where all these combiantions of symptoms occur is very low, so don't have sufficient data to cover all those scenarios
												% 			\item symptoms help to get knowledge about the data
												% 			\item is there a way to make us of they (symptoms) even tough such combination does not occur directly in the data, naive bayes tells way how to do it
												% 			\item don't try to find occurrences where they all occur together but tread them as occurring alone and independently
												% 			\item multiply because... Multiplying is that they are all occurring individually in separation
												% 			\item replacing joint conditional probability with independent probabilities
												% 			\item if want to make prediction on y, then denominator is not needed as term in denominator does not depend on y
												% 			\item can compute just nominator part above for covid positive and covid negative and then check which one has the highest value
												% 			\item can drop denominator because which of the value of y is more likely instead of computing the exact probability of values
												% 			\item formula tells probabilty of y given of some additional factors / symptoms x1 to xn is initial probablity of y times a correction factor, how much it helps to know that x1 occured, if x1 occurs often with y it is a high probability, it helps it increases belief to knowing that y occurs
												% 			\item maybe some of the factors will increase probability, some other of them will decrease, in the end total product will tell some information whether those additional factors / variables they will actually increase of decrease initial believe about y happening
												% 		\end{itemize}
												% 		\begin{center}
												% 			\color{SwitchColor}\rule{12cm}{0.25mm}
												% 		\end{center}
												% 		\begin{itemize}
												% 			\item while naive bayes iterated over all the data to compute probabilities conditional variables subject to the target and the probability of the Target or the probability of the of the conditional variable occurring however it did not store any logic it just looked up at the table
												% 		\end{itemize}
												% 	\end{minipage}
												% }
											}
										child {
												node {Bayes Rule
														% \resizebox{\textwidth}{!}{
														% 	% \begin{minipage}[t]{12cm}
														% 	% 	\begin{itemize}
														% 	% 		\item probability of y happening, have additional information x, know a little bit more what factors contribute to y, probability of y occuring will either increase / decrease based on how x will contribute to y
														% 	% 		\item how does ratio work, nominator says what is likelyhood for x occuring among the occurencies of y
														% 	% 		\item in other words, if x is more likely to occur with y then it is likely to occur alone, then it means x is a contributing factor to y because it occurs more often when y occurs
														% 	% 		\item mit x kommt weitere Variable dazu, und wenn man was über diese Variable weiß kann man diese Information nutzen als wenn man normal einfach ne weitere Variable hinzufügt und Faktor 1 (unabhängigkeit) annimmt bei den neuen Paaren
														% 	% 	\end{itemize}
														% 	% 	\begin{center}
														% 	% 		\color{SwitchColor}\rule{12cm}{0.25mm}
														% 	% 	\end{center}
														% 	% 	\begin{itemize}
														% 	% 		\item are those factors if they occur alone more frequent or less frequent when they occur together with a value of y
														% 	% 		\item man tut von der conditional variable alle entsprechenden cases nehmen und schauen wie oft die eigentliche Variable darin ihren entsprechenden Wert hat
														% 	% 	\end{itemize}
														% 	% \end{minipage}
														% }
													}
											}
									}
							}
					}
				child {
						node (loss) {Loss Functions}
						child {
								node {Maximum Likelihood Estimation
										% \resizebox{\textwidth}{!}{
										% 	\begin{minipage}[t]{12cm}
										% 		\begin{itemize}
										% 			\item probabilistic logic behind loss functions
										% 			\item minimizing least square error is equivalent to maximizing likelihood of observing the target given the data
										% 			\item and the mean of the errors is located at the estimated target
										% 			\item on average the highest density would be at the predicted value and then there's where you would have the smallest likelihood for an error, at the point of the estimation
										% 			\item mean is prediction of this linear model
										% 			\item mean value of predictions is going to be output of prediction model
										% 			\item likelihood of predicting y given X and the prediction model should be as large as possible
										% 			\item should be large on all the measurements that we have seen so it should be large on the first data point given the first observed features and the predictions of a model and it should be high on the second data point it should be high as well on all the other data points
										% 			\item have to maximize this likelihood in order for our prediction model to be accurate in estimating y
										% 			\item average value in normal distribution replaced with output of prediction model
										% 			\item as you see here maximizing the probabilistic likelihood of observing the targets if you assume that the errors are normally distributed that is equivalent to just learning a prediction model that minimizes the least square error
										% 			\item what does it mean to optimize the prediction model to minimize the sum of the losses of observed instances in other words to minimize the amount of error that we are doing in our prediction
										% 		\end{itemize}
										% 	\end{minipage}
										% }
									}
							}
						child {
								node {Regression}
								child {
										node {Least Squares}
									}
								child {
										node {L1}
									}
							}
						child {
								node {(Multi-class) Classification}
								child {
										node {Logloss}
										child {
												node {Softmax
														% \resizebox{\textwidth}{!}{
														% 	\begin{minipage}[t]{12cm}
														% 		\begin{itemize}
														% 			\item re-exrpess target variable as being as many binary targets as there categories
														% 			\item learrn one prediction model for every category
														% 			\item then connect together with (this) formula
														% 			\item prediction for class c and then normalize by sum of probabilies of all classes together
														% 			\item that ensures us that we have probability distribution of C
														% 			\item sum of probabilities is going to sum to 1
														% 			\item each between 0 and 1, because every component smaller than sum of all components
														% 			\item how well does proability for the 6th class match the true target of the sixth class, is either 1 or 0 because of definition above
														% 			\item if is 1 then want this log part be close to 0 by making ync be close to 1
														% 			\item this means this loss function is going to be small only in cases where prediction matches exactly the case of the instance of the class where it is one
														% 		\end{itemize}
														% 	\end{minipage}
														% }
													}
											}
									}
								child {
										node {Binary Classification}
										child {
												node {Logistic Loss
														% \resizebox{\textwidth}{!}{
														% 	\begin{minipage}[t]{12cm}
														% 		\begin{itemize}
														% 			\item if is 1, left handside, log($\hat y$) goes to 0 if $\hat y$ goes to 1, for values that go towards 0 the logarithm will be negative quantity, - in front, so large positive value, measure high error in cases where $\hat y$ is 0 and true y was one, in cases where true y was 1 and $\hat y$ was close to 1, log(1) comes 0, so do 0 error
														% 		\end{itemize}
														% 	\end{minipage}
														% }
													}
											}
										child {
												node {Hinge Loss}
											}
									}
							}
					}
				child {
						node {Regularisation}
						child {
								node {Underfitting}
							}
						child {
								node {Overfitting}
							}
					}
				child {
						node {Objective Function}
					}
			}
		child {
				node {Unsupervised Learning}
			}
		child {
				node {Design Cycle
						% \resizebox{\textwidth}{!}{
						% 	\begin{minipage}[t]{14cm}
						% 		\begin{itemize}
						% 			\item those three steps here the steps before applying machine learning are actually crucial they're crucial because if you don't do them appropriately then your machine learning models will will not be very successful they will not be very accurate so you must clean the data well you must do diligent feature extraction and encoding and you must apply feature selection if it is necessary
						% 			\item we can merge those four steps (at the beginnning) in what we call end to end learning for example deep learning is typically one way of doing of doing end-to-end learning still in deep learning sometimes you have to do pre-processing but in essence the concept of end-to-end learning is that you provide the data in a in an as original manner as possible as you collected and then the feature extraction and the feature selection is done by the same model that does the prediction
						% 			\item ... so what it means is that the prediction it means is that the prediction model incorporates it the feature representation
						% 			\item ... which means it incorporates the extraction and the selection steps together
						% 			\item sometimes the prediction model by the nature of the way it is designed for example as a deep neural network might not need a lot of pre-processing so if you have lots of data and if your model is end to end learn end to end it is set up as an endtoend learning then you might need little pre-processing but pre-processing is is often used in practice because it improves accuracy
						% 		\end{itemize}
						% 		\begin{center}
						% 			\vspace{-0.35cm}
						% 			\color{SwitchColor}\rule{13cm}{0.25mm}
						% 		\end{center}
						% 		\begin{itemize}
						% 			\item the evaluation and model evaluation and model selection can also be more generic than that well in instance can be what if I don't know what machine learning algorithm do I use that is the part that we are we are actually also figuring out here so we can try a different machine learning algorithm and that means we move back to this step try this machine learning algorithm train it evaluate it measure its accuracy and calibrate its hyperparameters or its settings find the best settings for an algorithm and then you can try another algorithm uh maybe try first a linear model and see if you calibrate the the hyper parameters what accuracy you achieve then maybe you try a decision tree then maybe you try a neural network then maybe you try support Vector machine and in the end you keep the one that that works the best
						% 		\end{itemize}
						% 	\end{minipage}
						% }
					}
				child {
						node {1. Preprocessing
								% \resizebox{\textwidth}{!}{
								% 	\begin{minipage}[t]{12cm}
								% 		\begin{itemize}
								% 			\item in real life not offered clean data like matrix x that was in the dimensionality of n instances and m features, data scattered in relational data base that is found in different tables, find out way how to join all the features with primary keys of the tables
								% 			\item unbalanced target, cancer detection, 99% don't have it, very few cases to learn to classify the cancer, oversample minorty class, donwsample majority class
								% 			\item standardize data, maybe one column is normally distributed, maybe other column is not normally distributed, so might want to make all of them have a similar range for example that the column values they are then uh standardized to have a mean zero and a standard deviation of one
								% 		\end{itemize}
								% 	\end{minipage}
								% }
							}
					}
				child {
						node {2. Feature extraction / encoding
								% \resizebox{\textwidth}{!}{
								% 	\begin{minipage}[t]{12cm}
								% 		\begin{itemize}
								% 			\item that you need to convert features which originally are not in a vector format you need to convert them to a vector format
								% 			\item want to engineer features, use the weight and the height Square in order to create a new feature
								% 			\item sometimes those features are not uh they are not defined in a continuous
								% 			\item domain for example you might have uh the city as a feature if you want to for example predict the price of a house and you have the location where the house is actually situated and that could be Berlin or friborg or London and these are not numerical features that you can train a prediction model so you have to convert them to some numerical feature and one way to handle it is the one hot encoding of categorical features so if you have 10 categories then instead of this 10 category features you create 10 different features each being either zero or one for example  location Berlin one it is if it is Berlin zero if it is not so you create one feature per each category and value zero or one if that particular value uh if that particular instance has the respective binary feature or not we call that one hot encoding of categorical features
								% 		\end{itemize}
								% 	\end{minipage}
								% }
							}
					}
				child {
						node {3. Feature selection
								% \resizebox{\textwidth}{!}{
								% 	\begin{minipage}[t]{12cm}
								% 		\begin{itemize}
								% 			\item we have many settings for the machine learning models and all the settings of those machine learning models which which we need to figure out a way how to handle they are part of the step of evaluation and model selection so we need to select what algorithm do I use how do I set those settings how do I design my experimental protocol with data in order to conduct that
								% 			\item you might have one feature for every product in your catalog and if you have millions of products well then you have lots of features and in essence sometimes for uh scalability reasons or also for accuracy reason reasons we might want to select only a subset of those features we call that a variable of feature subset selection process
								% 			\item some algorithms actually do it on their own for example decision trees so the step of feature selection is something that you don't always have to do uh you have to do it depending on what machine learning model you have to use and that comes with experience after you learn the characteristics of the prediction model then you can understand whether they do feature selection or not and then you decide whether to apply this step or not in the pipeline of Designing machine learning models
								% 		\end{itemize}
								% 	\end{minipage}
								% }
							}
					}
				child {
						node {4. Machine learning
								% \resizebox{\textwidth}{!}{
								% 	\begin{minipage}[t]{12cm}
								% 		\begin{itemize}
								% 			\item once you have finally a set of relevant features that have been extracted and pre-processed then you can train a prediction model that is the Box called that we are labeled labeling as machine learning that's the part where you define a prediction model and also you train the parameters of the prediction model in order to accurately estimate a Target variable
								% 			\item designing prediction models and actually optimizing the parameters of the prediction models in order to minimize a loss function
								% 		\end{itemize}
								% 	\end{minipage}
								% }
							}
					}
				child {
						node {5. Evaluation \& model selection
								% \resizebox{\textwidth}{!}{
								% 	\begin{minipage}[t]{16cm}
								% 		\begin{itemize}
								% 			\item we have to make sure that the models we train are carefully calibrated to achieve the highest accuracy
								% 			\item for example if you work with neuron networks you want to control the capacity the number of layers so how deep this networks have to be for some simpler tasks they don't have to be very deep for very complex tasks they need to be very deep so depending on the complexity of the task also the complexity of the prediction model has to be adapted and adapted means it needs to be configured
								% 			\item so this step is the step where we configure actually the settings of a prediction model which we otherwise call them hyperparameters
								% 			\item so remember prediction model has has parameters those are the thetas that we use to make prediction for instance in the linear model or the polinomial model
								% 			\item hyperparameters is a term that means those are parameters but they are not used to make prediction instead they are settings of the of the prediction model that's why we use
								% 			\item if you carefully select the hyper parameters then you achieve what we call generalization that the model can actually accurately predict the target value of new data
								% 			\item we don't want just a model that can correctly or accurately estimate the target variable of our previous collected data that is important of course because we learn from it however we don't want to to overfeed or we don't want to to actually feed to the noise of the recorded data so we want the model to be successful in the future because that's the purpose of machine learning we learn from the past so we can be accurate in the future and that is what we call generalization that we can generalize to new instances or generalize to Future instances and we we can design experiments how to control that we we are actually generalizing well to unseen instances
								% 		\end{itemize}
								% 	\end{minipage}
								% }
							}
					}
				child {
						node {6. Postprocessing
								% \resizebox{\textwidth}{!}{
								% 	\begin{minipage}[t]{12cm}
								% 		\begin{itemize}
								% 			\item if you have for example a sensitive attribute so let's say your attributes were were from X1 X2 until XM and actually one of those attributes is sensitive it could be for instance Ray so what what you want is is you want that the probability of estimating the target given if X2 the race is maybe is maybe africanamerican that this probability of Y given um it's equal to the probability of Y if X2 is maybe white American so you want to make sure that the that the estimations are done in way that those two probabilities U are as close as possible and that you can do by actually uh updating or um modifying the predictions in a way that that such fairness criterias hold
								% 			\item and for example some models don't yield  probabilities for example you might have models such as let's say support Vector machines which don't yield probababilities so you might want to adapt the predictions in a way that that they achieve probabilities and that is another post-processing step
								% 		\end{itemize}
								% 	\end{minipage}
								% }
							}
					}
			}
	\end{mindmapcontent}
	\begin{edges}
		\edge{loss}{target}
	\end{edges}
	% \annotation{test}{annotation}
\end{mindmap}
\end{document}
