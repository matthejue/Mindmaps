\documentclass[landscape, a4paper]{article}

\input{./content/packages}
\input{./content/desgin}
\input{./content/declarations}

\begin{document}
\fontsize{3pt}{3pt}\selectfont

\begin{minipage}[t]{0.2\linewidth}
	\fbox{General}
	\begin{betterlist}
		\item ML \alert{replaces the agent} with a math function $f$
		\begin{betterlist}
			\item an agent whose context are \alert{features} $x$ and its decision is the \alert{target} $y$
			\item \alert{trains} $f$ from \alert{observations} $\{(x_1 , y_1), \ldots , (x_N , y_N )\}$  such that $f(x_i) \approx y_i$, $\forall i \in \{1, \ldots , N\}$
		\end{betterlist}
	\end{betterlist}
	\fbox{Unsupervised Learning}
	\begin{betterlist}
		\item Target contains no explicit labels of the context features
		\item Clustering, Dimensionality reduction, Anomaly/Outlier Detection
	\end{betterlist}
	\fbox{Supervised Learning}
	\begin{betterlist}
		\item Target is given by an expert (ground-truth)
		\item Classification, Regression, Ranking
		\item \aalert{Archetype:}
		\begin{betterlist}
			\item \alert{data dimensions:} $N$ instances having $M$ features
			\item \alert{features:} $x \in \mathbb{R}^{N\times M}$ and \alert{target:} $y \in \mathbb{R}^N$
			\item \alert{prediction model:} having parameters $ \theta \in R^k$ is $f:\mathbb{R}^M\times \mathbb{R}^K \rightarrow \mathbb{R}$
			\begin{betterlist}
				\item $\hat y_n := f(x_n, \theta)$, function that predicts the target
			\end{betterlist}
			\item \alert{loss function:} $\mathcal{L}(y_n, \hat y_n): \mathbb{R}\times\mathbb{R}\rightarrow \mathbb{R}$
			\begin{betterlist}
				\item quality of a prediction model $f(x, \theta)$
				\item difference between the \alert{estimated target} $\hat y$ and \alert{ground-truth target} $y$
				\item the term loss is used for minimization tasks, e.g. regression, sometimes a maximization of $\mathcal{L}(y, \hat y)$ is needed
			\end{betterlist}
			\item \alert{regularisation:} $\Omega(\theta): \mathbb{R}^k \rightarrow \mathbb{R}$, fights overfitting to the noise of the measured data
			\item \alert{objective function:} $\displaystyle \underset{\theta}{argmin} \sum^N_{n=1} \mathcal{L}(y_n, \hat y_n) + \Omega(\theta)$
		\end{betterlist}
		\item \script{22}{Supervised Learning Example}
	\end{betterlist}
	\fbox{Prediction Models}
	\begin{betterlist}
		\item \aalert{Linear Model}
		\begin{betterlist}
			\item $\displaystyle \hat{y}_n=\theta_0+\theta_1 x_{n, 1}+\theta_2 x_{n, 2}+\cdots+\theta_M x_{n, M}=\theta_0+\sum_{m=1}^M \theta_m x_{n, m}$
		\end{betterlist}
		\item \aalert{Polynomial Regression}
		\begin{betterlist}
			\item $\displaystyle\hat{y}_n=\theta_0+\sum_{m=1}^M \theta_m x_{n, m}+\sum_{m=1}^M \sum_{m^{\prime}=1}^M \theta_{m, m^{\prime}} x_{n, m} x_{n, m^{\prime}}+\ldots$
		\end{betterlist}
		\item \aalert{Decision Trees}
		\begin{betterlist}
			\item \script{28}{as a Prediction Model}
			\item \script{29}{as a Step-wise Function}
		\end{betterlist}
		\item \aalert{Neural Networks}
		\begin{betterlist}
			\item Composite functions, i.e., functions of functions
			\item A neuron indexed i is a non-linear function $f_i(x, \theta_i)$
			\item If neuron $i$ is connected to neuron $j$ the model is $f_j (f_i (x, \theta_i ), \theta_j)$
			\item \script{30}{Example}
		\end{betterlist}
	\end{betterlist}
	\fbox{Loss Functions}
	\begin{betterlist}
		\item \aalert{Regression}
		\begin{betterlist}
			\item target is real-valued $y_n\in \mathbb{R}$
			\item \alert{Least-squares:} $\mathcal{L}(y_n, \hat y_n) := (y_n - \hat y_n)^2$
			\item \alert{L1:} $\mathcal{L}(y_n, \hat y_n) := |y_n - \hat y_n|$
		\end{betterlist}
		\item \aalert{Multi-class Classification}
		\begin{betterlist}
			\item Re-express targets $y_n \in \{1, \ldots , C\}$ as one-vs-all, i.e. $y_{n, c} := \begin{cases}
					1 & y_n = C    \\
					0 & y_n \neq C
				\end{cases}$
			\item Learn model parameters per class $\theta \in \mathbb{R}^{C\times K}$
			\item Estimations expressed as probabilities among classes: $\displaystyle\hat y_{n,c} = \frac{e^{f(x_n,\uptheta_c)}}{\sum^{C}_{q=1} e^{f(x_n,\uptheta_q)}}$
			\item Logloss: $\displaystyle L(y_{n,:}, ˆy_{n,:}) := −\sum^{C}_{c=1} y_{n,c} log(\hat y_{n,c})$
			\item \aalert{Binary Classification}
			\begin{betterlist}
				\item \alert{Logistic loss}, $y_n\in\{0, 1\}$:
				\begin{betterlist}
					\item $\mathcal{L}(y_n, \hat y_n) := -y_n log(\hat y_n) - (1 - y_n)log(1-\hat y_n)$
					\item is \alert{convex}, but there's no analytic solution
					\item \alert{as case destinction:}\\
					\begin{betterlist}
						\item $J\left(h_{\mathbf{w}}\left(y_i, \mathbf{x}_i\right)\right)=\left\{\begin{array}{rll}
								-\log \left(h_{\mathbf{w}}\left(\mathbf{x}_i\right)\right)   & \text { for } & y_i=1 \\
								-\log \left(1-h_{\mathbf{w}}\left(\mathbf{x}_i\right)\right) & \text { for } & y_i=0\end{array}\right)$
					\end{betterlist}
				\end{betterlist}
				\item \alert{Hinge loss}, $y_n\in\{-1,1\}$:
				\begin{betterlist}
					\item $\mathcal{L}(y_n, \hat y_n) := max(0, 1 - y_n\hat y_n)$
				\end{betterlist}
			\end{betterlist}
		\end{betterlist}
	\end{betterlist}
	\fbox{Generalization Performance}
	\begin{betterlist}
		\item \alert{Overfitting} (High model variance): Model perfectly fits the training data (incl. noise). Captering noise.
		\item \alert{Underfitting} (High model bias): Model fails to fit the training data. Unable to capture complexity
		\item \alert{Generalization}: Model is accurate on test data
		\item \script{33}{Example (f.)}
	\end{betterlist}
	\fbox{Probability Theory}
	\begin{betterlist}
		\item \script{36}{Example with linear model (ff.)}
		\item \aalert{Maximum Likelihood Estimation}
		\begin{betterlist}
			\item \alert{likelihood} of observing the \alert{target} $y \in \mathbb{R}^N$ is $\displaystyle \mathcal{L}(\theta) = \prod^N_{n=1} \hat p(y_n | x_n, \theta)$
			\begin{betterlist}
				\item $\hat p(y |x, \theta)$ is the \alert{probability density function} for the target $y$ given features $x$ and parameters $\theta$
				% \item assume the error in predicting the ground truth yn is normally distributed: $\epsilon | x \sim \mathcal{N}(0, \sigma^2)$, so $\hat y \sim \mathcal{N}(\theta_0 + \sum^M_{m=1} \theta_m x_m, \sigma^2)$, because of the linear model: $\displaystyle \hat y = \theta_0 + \sum^M_{m=1} \theta_m x_m$
			\end{betterlist}
			\item \alert{Aim:} Estimate the $\theta$-s which maximize the likelihood
			\begin{betterlist}
				\item $\underset{\theta}{\operatorname{argmax}}\;g(\theta)=\underset{\theta}{\operatorname{argmax}}\;\log (g(\theta))$
				\item $\displaystyle log\;\mathcal{L}(\theta) = \log \prod_{n=1}^N \hat{p}\left(y_n \mid \theta\right)=\sum_{n=1}^N \log \left(\hat{p}\left(y_n \mid \theta\right)\right)$
			\end{betterlist}
		\end{betterlist}
		\item \script{49}{Nearest Neighbors Example (ff.)}
		\item \aalert{Bayes Rule}
		\begin{betterlist}
			\item \underline{assume $x \in X, y \in Y$:}
			\begin{betterlist}
				\item $\displaystyle P(y | x) = P(y)\frac{P(x | y)}{P(x)} = P(y) \frac{P(x | y)}{\sum_{y′\in Y} P(x | y')P(y')}$
			\end{betterlist}
			\item \alert{Multiple conditional variables:}

			$\displaystyle P(y | x_1, x_2 \ldots , x_M) = P(y)\frac{P(x_1, x_2 \ldots , x_M | y)}{P(x_1, x_2 \ldots, x_M)}$
			\item \script{54}{COVID-19 Example (ff.)}
			\item \alert{Naive Bayes}
			\begin{betterlist}
				\item assume $x_1, x_2 \ldots, x_M$ are all independent given $y$ :\\ $\begin{aligned}[t]
						P\left(y \mid x_1, x_2 \ldots, x_M\right) & =P(y) \frac{P\left(x_1, x_2 \ldots, x_M \mid y\right)}{P\left(x_1, x_2 \ldots, x_M\right)}                                         \\
						                                          & =P(y) \frac{P\left(x_1 \mid y\right) P\left(x_2 \mid y\right) \ldots P\left(x_M \mid y\right)}{P\left(x_1, x_2 \ldots, x_M\right)}
					\end{aligned}$
				\item if the goal is only to predict $y$ we can drop the denominator:\\ $P\left(y \mid x_1, x_2 \ldots, x_M\right) \propto P(y) P\left(x_1 \mid y\right) P\left(x_2 \mid y\right) \ldots P\left(x_M \mid y\right)$
			\end{betterlist}
		\end{betterlist}
	\end{betterlist}
	\fbox{Machine Learning Design Cycle}

	\includegraphics[width=0.8\linewidth, center]{./figures/ml_design_cycle.png}
	\begin{betterlist}
		\item \aalert{Preprocessing}
		\begin{betterlist}
			\item \alert{Collecting the data}
			\begin{betterlist}
				\item Design sensor systems, design questionaires
				\item Invest in labeling, get legal approval to use data, etc.
			\end{betterlist}
			\item \alert{Cleaning the data}
			\begin{betterlist}
				\item Standardization
				\item Missing values
				\item Outliers
				\item Imbalanced target
			\end{betterlist}
		\end{betterlist}
		\item \aalert{Feature Extraction \& Encoding}
		\begin{betterlist}
			\item \alert{Extract features:}
			\begin{betterlist}
				\item \ldots from a human cell, a text, a sound, a brainwave, an image
			\end{betterlist}
			\item \alert{Engineer features:}
			\begin{betterlist}
				\item E.g., body mass index: BMI = weight/ (height2)
			\end{betterlist}
			\item \alert{Encode features:}
			\begin{betterlist}
				\item One-hot encoding of categorical features
			\end{betterlist}
		\end{betterlist}
		\item \aalert{Feature Selection}
		\begin{betterlist}
			\item E.g. recommendation system datasets have millions of features
			\item certain methods incorporate ’automatic’ feature selection, others do not scale in terms of the number of features
		\end{betterlist}
		\item \aalert{Machine Learning}
		\begin{betterlist}
			\item Models: 'core' of machine learning
			\item \alert{In practice} features are as important as models
		\end{betterlist}
		\item \aalert{Evaluation and Model Selection}
		\begin{betterlist}
			\item Tuning the settings of models (hyper-parameters)
			\item \ldots ensures generalization.
		\end{betterlist}
		\item \aalert{Post-processing}
		\begin{betterlist}
			\item Ensure \alert{fairness} / avoid algorithmic bias
			\begin{betterlist}
				\item E.g., model should not discriminate based on race, gender, religion, sexual orientation, \ldots
			\end{betterlist}
			\item Some models don’t yield proper probabilities
			\begin{betterlist}
				\item Transform model outputs to probabilities
			\end{betterlist}
		\end{betterlist}
	\end{betterlist}
\end{minipage}
\begin{minipage}[t]{0.2\linewidth}
	\fbox{Linear methods}
	\begin{betterlist}
		\item \aalert{Linear Regression}
		\begin{betterlist}
			\item $f\left(\mathbf{x}_i ; \mathbf{w}\right)=w_0+w_1 x_{i, 1}+\ldots+w_D x_{i, D}=w_0+w^T x_i$
			\item non-linear function can be approximated by a linear function if one maps the feature x into a new dimensionality with a \alert{basis function}: $\phi: \mathbb{R}^D\rightarrow \mathbb{R}^M$
			\item \alert{enhanced linear regression model:} linear combinations of fixed \alert{nonlinear functions} of the input variables: $\displaystyle f(x_i, \omega) = \omega_0 + \sum^{M}_{j=1} \omega_j\phi(x_i)_j$
			\begin{betterlist}
				\item $f(x, w)$ is still a \alert{linear function} of the weights / parameters $w_i$
				\item $f(x, w)$ is a \alert{nonlinear} function of the input dimensions / variables $x_i$
				\item Analytical solution
				\begin{betterlist}
					\item setting the \alert{partial derivatives} w.r.t. $w$ to zero: $\displaystyle\dfrac{\partial}{\partial \omega} \sum^N_{n=1} \mathcal{L}(y_n, f_n(x, \omega)) = 0$
					\item solving for $\omega$
				\end{betterlist}
			\end{betterlist}
		\end{betterlist}
		\item \aalert{Linear Classification}
		\begin{betterlist}
			\item using \alert{logistic regression} which bounds the output of linear regression to $0 \le h_w(x) \le 1$
			\begin{betterlist}
				\item e.g. $h_w(x) = g(w^Tx)$ with \alert{sigmoid} function or \alert{logistic function} $\displaystyle g(z) = \frac{1}{1 + e^{-z}}$ introduces \alert{non-linearity}
				\item $h_w(x) = P(y=1|x; \omega)$, estimated probability, that $y = 1$
			\end{betterlist}
			\item Gradient Descent
			\begin{betterlist}
				\item  if there is \alert{no analytic solution}
				\item start with a guess for $\omega$
				\item move $\omega$ towards the minimum of $J(\omega)$
				\item minimize $J(\omega)$ by updating $\omega$ in the negative direction of $\dfrac{\partial J(\omega)}{\partial\omega}$: $\displaystyle\omega^{next} \leftarrow \omega^{prev} - \eta \frac{\partial J(\omega)}{\partial\omega^{prev}} $
				\item Gradients
				\begin{betterlist}
					\item for \alert{linear regression}
					\item for \alert{logistic regression}
				\end{betterlist}
				\item Stochastic Gradient Descent
				\begin{betterlist}
					\item for \alert{large dimensions}
					\item nice if the error function is \alert{convex}
				\end{betterlist}
			\end{betterlist}
		\end{betterlist}
	\end{betterlist}
\end{minipage}
\begin{minipage}[t]{0.2\linewidth}
\end{minipage}
\begin{minipage}[t]{0.2\linewidth}
\end{minipage}
\begin{minipage}[t]{0.2\linewidth}
\end{minipage}

\newpage

\begin{minipage}[t]{0.2\linewidth}
\end{minipage}
\begin{minipage}[t]{0.2\linewidth}
\end{minipage}
\begin{minipage}[t]{0.2\linewidth}
\end{minipage}
\begin{minipage}[t]{0.2\linewidth}
\end{minipage}
\begin{minipage}[t]{0.2\linewidth}
\end{minipage}

\end{document}
